---
title: "Spark Interface for R"
output:
  github_document:
    fig_width: 9
    fig_height: 5
---

[![Travis-CI Build Status](https://travis-ci.com/rstudio/rspark.svg?token=MxiS2SHZy3QzqFf34wQr&branch=master)](https://travis-ci.com/rstudio/rspark)

A set of tools to provision, connect and interface to Apache Spark from within the R language and ecosystem. This package supports connecting to local and remote Apache Spark clusters and provides support for R packages like dplyr and DBI.

```{r setup, include = FALSE}
knitr::opts_chunk$set(warning = FALSE, cache = FALSE, fig.path='res/')
library(rspark)
library(dplyr)
library(ggplot2)
```


## Installation

You can install the development version of the **spark** package using **devtools** as follows (note that installation of the development version of **devtools** itself is also required):

```{r, eval=FALSE}
devtools::install_github("hadley/devtools")
devtools::install_github("rstudio/rspark", auth_token = "b0a1ea10dd9a5c48a9c6ffdb7c3fc1aac04f4dce")
```

You can then install various versions of Spark using the `spark_install` function:

```{r}
library(rspark)
spark_install(version = "1.6.1", hadoop_version = "2.6", reset = TRUE)
```


## dplyr Interface

The spark package implements a dplyr back-end for Spark. Connect to Spark using the `spark_connect` function then obtain a dplyr interface using `src_spark` function:

```{r connection, message=FALSE}
# connect to local spark instance and get a dplyr interface
library(rspark)
library(dplyr)
sc <- spark_connect("local", cores = "auto", version = "1.6.1")
db <- src_spark(sc)

# copy the flights table from the nycflights13 package to Spark
copy_to(db, nycflights13::flights, "flights")
flights <- tbl(db, "flights")

# copy the Batting table from the Lahman package to Spark
copy_to(db, Lahman::Batting, "batting")
batting <- tbl(db, "batting")
```

Then you can run dplyr against Spark:

```{r}
# filter by departure delay and print the first few records
flights %>% filter(dep_delay == 2)
```

[Introduction to dplyr](https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html) provides additional dplyr examples you can try. For example, consider the last example from the tutorial which plots data on flight delays:

```{r ggplot2}
delay <- flights %>% 
  group_by(tailnum) %>%
  summarise(count = n(), dist = mean(distance), delay = mean(arr_delay)) %>%
  filter(count > 20, dist < 2000, !is.na(delay)) %>%
  collect

# plot delays
library(ggplot2)
ggplot(delay, aes(dist, delay)) +
  geom_point(aes(size = count), alpha = 1/2) +
  geom_smooth() +
  scale_size_area(max_size = 2)
```


### Window Functions

dplyr [window functions](https://cran.r-project.org/web/packages/dplyr/vignettes/window-functions.html) are also supported, for example:

```{r}

batting %>%
  select(playerID, yearID, teamID, G, AB:H) %>%
  arrange(playerID, yearID, teamID) %>%
  group_by(playerID) %>%
  filter(min_rank(desc(H)) <= 2 & H > 0)

```


## EC2

To start a new 1-master 1-slave Spark cluster in EC2 run the following code:

```{r, eval=FALSE}
library(rspark)
ci <- spark_ec2_cluster(access_key_id = "AAAAAAAAAAAAAAAAAAAA",
                        secret_access_key = "1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1",
                        pem_file = "spark.pem")

spark_ec2_deploy(ci)

spark_ec2_web(ci)
spark_ec2_rstudio(ci)

spark_ec2_stop(ci)
spark_ec2_destroy(ci)
```

The `access_key_id`, `secret_access_key` and `pem_file` need to be retrieved from the AWS console.

For additional configuration and examples read: [Using RSpark in EC2](docs/ec2.md)

## Extensibility

Spark provides low level access to native JVM objects, this topic targets users creating packages based on low-level spark integration. Here's an example of an R `count_lines` function built by calling Spark functions for reading and counting the lines of a text file.

```{r}
library(magrittr)

# define an R interface to Spark line counting
count_lines <- function(scon, path) {
  spark_context(scon) %>%
    spark_invoke("textFile", path) %>%
    spark_invoke("count")
}

# write a CSV 
tempfile <- tempfile(fileext = ".csv")
write.csv(nycflights13::flights, tempfile, row.names = FALSE, na = "")

# call spark to count the lines
count_lines(sc, tempfile)
```

Package authors can use this mechanism to create an R interface to any of Spark's underlying Java APIs.


## dplyr Utilities

You can cache a table into memory with:

```{r, eval=FALSE}
tbl_cache(db, "batting")
```

and unload from memory using:

```{r, eval=FALSE}
tbl_uncache(db, "batting")
```


## Connection Utilities

You can view the Spark web console using the `spark_web` function:

```{r, eval=FALSE}
spark_web(sc)
```

You can show the log using the `spark_log` function:

```{r}
spark_log(sc, n = 10)
```

Finally, we disconnect from Spark:

```{r}
spark_disconnect(sc)
```

## Additional Resources

For performance runs under various parameters, read: [RSpark Performance](docs/perf.md)
