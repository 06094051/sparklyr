---
title: "Spark Interface for R"
output:
  github_document:
    fig_width: 9
    fig_height: 5
---

A set of tools to provision, connect and interface to Apache Spark from within the R language and ecosystem. This package supports connecting to local and remote Apache Spark clusters and provides support for R packages like dplyr and DBI.

```{r setup, include = FALSE}
knitr::opts_chunk$set(warning = FALSE, cache = FALSE, fig.path='res/')
library(sparklyr)
library(ggplot2)
library(nycflights13)
library(Lahman)
```


## Installation

You can install the development version of the **sparklyr** package using **devtools** as follows (note that installation of the development version of **devtools** itself is also required):

```{r, eval=FALSE}
devtools::install_github("hadley/devtools")
devtools::reload(devtools::inst("devtools"))

devtools::install_github("rstudio/sparklyr", auth_token = "56aef3d82d3ef05755e40a4f6bdaab6fbed8a1f1")
```

You can then install various versions of Spark using the `spark_install` function:

```{r, eval=FALSE}
library(sparklyr)
spark_install(version = "1.6.1", hadoop_version = "2.6", reset = TRUE)
```


## dplyr Interface

The sparklyr package implements a dplyr back-end for Spark. Connect to Spark using the `spark_connect` function then use the returned connection as a remote dplyr source.

```{r connection, message=FALSE}
# connect to local spark instance 
library(sparklyr)
sc <- spark_connect("local", version = "1.6.1")
```

Now we copy some datasets from R into the Spark cluster:

```{r}
iris_tbl <- copy_to(sc, iris)
flights_tbl <- copy_to(sc, flights)
batting_tbl <- copy_to(sc, Batting, "batting")
src_tbls(sc)
```

Then you can run dplyr against Spark:

```{r}
# filter by departure delay and print the first few records
flights_tbl %>% filter(dep_delay == 2)
```

[Introduction to dplyr](https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html) provides additional dplyr examples you can try. For example, consider the last example from the tutorial which plots data on flight delays:

```{r ggplot2}
delay <- flights_tbl %>% 
  group_by(tailnum) %>%
  summarise(count = n(), dist = mean(distance), delay = mean(arr_delay)) %>%
  filter(count > 20, dist < 2000, !is.na(delay)) %>%
  collect

# plot delays
library(ggplot2)
ggplot(delay, aes(dist, delay)) +
  geom_point(aes(size = count), alpha = 1/2) +
  geom_smooth() +
  scale_size_area(max_size = 2)
```


### Window Functions

dplyr [window functions](https://cran.r-project.org/web/packages/dplyr/vignettes/window-functions.html) are also supported, for example:

```{r}

batting_tbl %>%
  select(playerID, yearID, teamID, G, AB:H) %>%
  arrange(playerID, yearID, teamID) %>%
  group_by(playerID) %>%
  filter(min_rank(desc(H)) <= 2 & H > 0)

```

## ML Functions

MLlib functions are also supported, see [ml samples](docs/ml_examples.md). For instasnce, k-means can be run as:

```{r}
model <- iris_tbl %>%
  select(Petal_Width, Petal_Length) %>%
  ml_kmeans(centers = 3)

iris_tbl %>%
  select(Petal_Width, Petal_Length) %>%
  collect %>%
  ggplot(aes(Petal_Length, Petal_Width)) +
    geom_point(data = model$centers, aes(Petal_Width, Petal_Length), size = 60, alpha = 0.1) +
    geom_point(aes(Petal_Width, Petal_Length), size = 2, alpha = 0.5)

```

## Reading and Writing Data

```{r}
  temp_csv <- tempfile(fileext = ".csv")
  temp_parquet <- tempfile(fileext = ".parquet")
  temp_json <- tempfile(fileext = ".json")
  
  spark_write_csv(iris_tbl, temp_csv)
  iris_csv_tbl <- spark_read_csv(sc, "iris_csv", temp_csv)
  
  spark_write_parquet(iris_tbl, temp_parquet)
  iris_parquet_tbl <- spark_read_parquet(sc, "iris_parquet", temp_parquet)
  
  spark_write_csv(iris_tbl, temp_json)
  iris_json_tbl <- spark_read_csv(sc, "iris_json", temp_json)
  
  src_tbls(sc)
```

## Extensibility

Spark provides low level access to native JVM objects, this topic targets users creating packages based on low-level spark integration. Here's an example of an R `count_lines` function built by calling Spark functions for reading and counting the lines of a text file.

```{r}
# define an R interface to Spark line counting
count_lines <- function(sc, path) {
  spark_invoke(sc, "textFile", path, as.integer(1)) %>% 
    spark_invoke("count")
}

# write a CSV 
tempfile <- tempfile(fileext = ".csv")
write.csv(nycflights13::flights, tempfile, row.names = FALSE, na = "")

# call spark to count the lines
count_lines(sc, tempfile)
```

Package authors can use this mechanism to create an R interface to any of Spark's underlying Java APIs.


## dplyr Utilities

You can cache a table into memory with:

```{r, eval=FALSE}
tbl_cache(sc, "batting")
```

and unload from memory using:

```{r, eval=FALSE}
tbl_uncache(sc, "batting")
```


## Connection Utilities

You can view the Spark web console using the `spark_web` function:

```{r, eval=FALSE}
spark_web(sc)
```

You can show the log using the `spark_log` function:

```{r}
spark_log(sc, n = 10)
```

Finally, we disconnect from Spark:

```{r}
spark_disconnect(sc)
```

## Additional Resources

For performance runs under various parameters, read: [Dplyr Performance](docs/perf_dplyr.md) and [Spark 1B-Rows Performance](docs/perf_1b.md)
