---
title: "RSpark Performance: 1B Rows"
output:
  github_document:
    fig_width: 9
    fig_height: 5
---

## Setup
```{r}
library(rspark)
library(magrittr)

spark_install(version = "2.0.0-SNAPSHOT", reset = TRUE, logging = "WARN")
sc <- spark_connect(master = "local", version = "2.0.0-preview", memory = "12G ")

ses <- spark_invoke_static_ctor(sc, "org.apache.spark.sql.SparkSession", spark_context(sc)) %>%
    spark_invoke("builder") %>%
    spark_invoke("master", "local") %>%
    spark_invoke("appName", "1btest") %>%
    spark_invoke("getOrCreate")

spark_conf <- function(ses, config, value) {
  ses %>%
    spark_invoke("conf") %>%
    spark_invoke("set", config, value)
}

spark_sum_range <- function(scon) {
  billion <- spark_invoke_static_ctor(sc, "java.math.BigInteger", "1000000000") %>%
    spark_invoke("longValue")
  
  result <- ses %>%
    spark_invoke("range", as.integer(billion)) %>%
    spark_invoke("toDF", list("x")) %>%
    spark_invoke("selectExpr", list("sum(x)"))
  
  result
}

```

## Spark 1.0

```{r}
system.time({
  spark_conf(ses, "spark.sql.codegen.wholeStage", "false")
  result <- spark_sum_range(sc)
  sum <- spark_invoke(result, "collect")[[1]]
})

sum
```

## Spark 2.0

```{r}
system.time({
  spark_conf(ses, "spark.sql.codegen.wholeStage", "true")
  result <- spark_sum_range(sc)
  sum <- spark_invoke(result, "collect")[[1]]
})

sum
```

# Cleanup

```{r}
spark_disconnect(sc)
```
