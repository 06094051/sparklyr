---
title: "Spark Performance: 1B Rows"
output:
  github_document:
    fig_width: 9
    fig_height: 5
---

## Setup
```{r}
sparklyr:::spark_install(version = "2.0.0-SNAPSHOT", reset = TRUE, logging = "WARN")
```

## Initialization

```{r}
library(sparklyr)
library(sparkapi)
library(dplyr)
library(ggplot2)

parquetPath <- file.path(getwd(), "billion.parquet")

if (!file.exists(parquetPath)) {
  billion <- sparkapi_invoke_new(sc, "java.math.BigInteger", "1000000000") %>%
    sparkapi_invoke("longValue")
  
  ses %>%
    sparkapi_invoke("range", as.integer(billion)) %>%
    sparkapi_invoke("toDF") %>%
    sparkapi_invoke("write") %>%
    sparkapi_invoke("save", "billion.parquet")
}

spark_conf <- function(ses, config, value) {
  ses %>%
    sparkapi_invoke("conf") %>%
    sparkapi_invoke("set", config, value)
}

logResults <- function(label, test) {
  runTimes <- lapply(seq_len(3), function(idx) {
    runTime <- system.time({
      sum <- test()
    })
    
    as.data.frame(list(
      label = label,
      time = runTime[[3]],
      sum = sum))
  })
  
  runTimes <- do.call(rbind, runTimes)
  
  as.data.frame(list(
    label = label,
    min = min(runTimes$time),
    max = max(runTimes$time),
    mean = mean(runTimes$time)
  ))
}

sparkTest <- function(test, loadIntoDf = TRUE) {
  sc <- spark_connect(master = "local", version = "2.0.0-preview", memory = "12G")
  sparkSql <- sparkapi_invoke_new(
    sc,
    "org.apache.spark.sql.SQLContext",
    spark_context(sc)
  )
  
  ses <- sparklyr:::spark_sql_or_hive(sparklyr:::spark_api(sc))
  df <- NULL
  
  if (loadIntoDf) {
    df <- sparkSql %>%
      sparkapi_invoke("read") %>%
      sparkapi_invoke("parquet", list(parquetPath)) %>%
      sparkapi_invoke("repartition", as.integer(parallel::detectCores()))
    
    df %>%
      sparkapi_invoke("cache") %>%
      sparkapi_invoke("count")
  } else {
    invisible(
      spark_read_parquet(db, "billion", parquetPath, repartition = parallel::detectCores())
    )
  }
  
  result <- test(sc, db, ses, df)
  
  spark_disconnect(sc)
  result
}

```

## Tests

### Sum range from formula

```{r}
spark_sum_range <- function(sc, db, ses, df) {
  billion <- sparkapi_invoke_new(sc, "java.math.BigInteger", "1000000000") %>%
    sparkapi_invoke("longValue")
  
  result <- ses %>%
    sparkapi_invoke("range", as.integer(billion)) %>%
    sparkapi_invoke("toDF", list("x")) %>%
    sparkapi_invoke("selectExpr", list("sum(x)"))
    
  sparkapi_invoke(result, "collect")[[1]]
}
```

### Sum range from parquet

```{r}
spark_sum_range_parquet <- function(sc, db, ses, df) {
  df <- sparkapi_invoke(sparklyr:::spark_sql_or_hive(sparklyr:::spark_api(db)), "read") %>%
    sparkapi_invoke("parquet", list(parquetPath))
    
  result <- sparkapi_invoke(df, "selectExpr", list("sum(x)")) %>%
    sparkapi_invoke("collect")
  
  result[[1]]
}
```

### Sum range from memory

```{r}
spark_sum_range_mem <- function(sc, db, ses, df) {
  df %>%
    sparkapi_invoke("selectExpr", list("sum(x)")) %>%
    sparkapi_invoke("collect")
}
```

### Sum range using sparklyr

```{r}
spark_sum_range_sparklyr <- function(sc, db, ses, df) {
  tbl(db, "billion") %>%
    summarise(total = sum(x)) %>%
    collect
}
```

### Sum range using SparkR SQL

```{r}
spark_sum_range_sparkr_sql_prepare <- function() {
  installInfo <- sparklyr:::spark_install_info(sparkVersion = "2.0.0-preview", hadoopVersion = "2.6")
  
  Sys.setenv(SPARK_HOME = installInfo$sparkVersionDir)
  library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
  scR <- sparkR.init(master = "local[*]", sparkEnvir = list(spark.driver.memory="12G"))
  sqlContextR <- sparkRSQL.init(scR)
  df <- loadDF(sqlContextR, parquetPath, "parquet")
  df <- repartition(df, parallel::detectCores())
  
  registerTempTable(df, "billion")
  
  sql(sqlContextR, "CACHE TABLE billion")
  collect(sql(sqlContextR, "SELECT count(*) FROM billion"))
  
  sqlContextR
}

spark_sum_range_sparkr_sql <- function(sqlContextR) {
  collect(sql(sqlContextR, "SELECT sum(*) FROM billion"))
}

spark_sum_range_sparkr_terminate <- function() {
  sparkR.stop()
  detach(name = "package:SparkR")
}
```

### Sum range using SparkR Native

```{r}
spark_sum_range_sparkr_native_prepare <- function() {
  installInfo <- sparklyr:::spark_install_info(sparkVersion = "2.0.0-preview", hadoopVersion = "2.6")
  
  Sys.setenv(SPARK_HOME = installInfo$sparkVersionDir)
  library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
  scR <- sparkR.init(master = "local[*]", sparkEnvir = list(spark.driver.memory="12G"))
  sqlContextR <- sparkRSQL.init(scR)
  
  df <- loadDF(sqlContextR, parquetPath, "parquet")
  df <- repartition(df, parallel::detectCores())
  cache(df)
  count(df)
  
  df
}

spark_sum_range_sparkr_native <- function(df) {
  collect(summarize(df, total = sum(df$x)))
}
```

### Sum range using dplyr

```{r}
spark_sum_range_dplyr_prepare <- function() {
  df <- as.data.frame(as.numeric(seq_len(1000000000)))
  colnames(df) <- c("x")
  head(df)
  df
}

spark_sum_range_dplyr <- function(df) {
  df %>% summarise(sum(x))
}
```

## Results

### Sum range from formula using 1.6

```{r runOldCode}
runOldCode <- sparkTest(function(sc, db, ses, df) {
  logResults("1.6.1 Code", function() {
    spark_conf(ses, "spark.sql.codegen.wholeStage", "false")
    spark_sum_range(sc, db, ses, df)
  })
})
```

### Sum range from formula using 2.0

```{r runCode}
runCode <- sparkTest(function(sc, db, ses, df) {
  logResults("2.0.0 Code", function() {
    spark_conf(ses, "spark.sql.codegen.wholeStage", "true")
    spark_sum_range(sc, db, ses, df)
  })
})
```

### Sum range from parquet

```{r runParquet}
runParquet <- sparkTest(function(sc, db, ses, df) {
  logResults("2.0.0 Parquet", function() {
    spark_conf(ses, "spark.sql.codegen.wholeStage", "true")
    sum <- spark_sum_range_parquet(sc, db, ses, df)
  })
})
```

### Sum range from memory

```{r runInMem}
runInMem <- sparkTest(function(sc, db, ses, df) {
  logResults("2.0.0 In-Mem", function() {
    spark_conf(ses, "spark.sql.codegen.wholeStage", "true")
    sum <- spark_sum_range_mem(sc, db, ses, df)
  })
})
```

### Sum range using sparklyr

```{r runSparklyr}
runSparklyr <- sparkTest(function(sc, db, ses, df) {
  logResults("2.0.0 sparklyr", function() {
    spark_conf(ses, "spark.sql.codegen.wholeStage", "true")
    sum <- spark_sum_range_run_sparklyr(sc, db, ses, df)
  })
}, loadIntoDf = FALSE)
```

### Sum range using SparkR SQL

```{r runSparkRSQL}
sqlContextR <- spark_sum_range_sparkr_sql_prepare()
runSparkRSQL <- logResults("2.0.0 SparkR SQL", function() {
  sum <- spark_sum_range_sparkr_sql(sqlContextR)
})
spark_sum_range_sparkr_terminate()
```

### Sum range using SparkR native

```{r runSparkRNative}
dfSparkR <- spark_sum_range_sparkr_native_prepare()
runSparkRNative <- logResults("2.0.0 SparkR Native", function() {
  sum <- spark_sum_range_sparkr_native(dfSparkR)
})
spark_sum_range_sparkr_terminate()
```

### Sum range using dplyr

```{r runDplyr}
dplyrDf <- spark_sum_range_dplyr_prepare()
runDplyr <- logResults("dplyr", function() {
  sum <- spark_sum_range_dplyr(dplyrDf)
})
dplyrDf <- NULL
```

## Results

```{r allRuns}
allRuns <- lapply(
  list(
    runOldCode,
    runCode,
    runParquet,
    runInMem,
    runSparklyr,
    runSparkRSQL,
    runSparkRNative,
    runDplyr
  ),
  function(e) {
    colnames(e) <- c("label", "min", "max", "mean")
    e
  })
results <- do.call("rbind", allRuns)
```

### Results chart

```{r}
results %>% 
  ggplot(aes(label, mean)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(mean, 2)), vjust = -0.2, hjust = 1.1) +
  geom_errorbar(aes(ymin = min, ymax = max), width = 0.1)
```

### Results table

```{r}
results
```
