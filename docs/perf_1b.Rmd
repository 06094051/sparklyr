---
title: "RSpark Performance: 1B Rows"
output:
  github_document:
    fig_width: 9
    fig_height: 5
---

## Setup
```{r}
library(rspark)

spark_install(version = "2.0.0-preview", reset = TRUE, logging = "WARN")
sc <- spark_connect(master = "local", version = "2.0.0-preview")

spark_conf <- function(scon, config, value) {
  spark_context(scon) %>%
    spark_invoke("conf") %>%
    spark_invoke("set", config, value)
}

spark_sum_range <- function(scon, range) {
  spark_context(scon) %>%
    spark_invoke("range", range) %>%
    spark_invoke("selectExpr", "sum(id)") %>%
    spark_invoke("show")
}

```

## Spark 1.0

```{r}
system.time({
  spark_conf(sc, "spark.sql.codegen.wholeStage", FALSE)
  spark_sum_range(sc, 1000)
})
```

## Spark 2.0

```{r}
system.time({
  spark_invoke_static(sc, "org.apache.spark", "conf")
  spark.range(1000).selectExpr("sum(id)").show()
})
```

# Cleanup

```{r}
spark_disconnect(sc)
```
