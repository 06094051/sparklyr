% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dplyr_spark_data.R
\name{dplyr-spark-data}
\alias{dplyr-spark-data}
\alias{load_csv}
\alias{load_df}
\alias{load_json}
\alias{load_parquet}
\alias{save_csv}
\alias{save_json}
\alias{save_parquet}
\title{Loads a CSV file and provides a data source compatible with dplyr}
\usage{
load_csv(con, name, path, repartition = 0, memory = TRUE,
  overwrite = TRUE)

save_csv(x, path)

load_parquet(con, name, path, repartition = 0, memory = TRUE,
  overwrite = TRUE)

save_parquet(x, path)

load_json(con, name, path, repartition = 0, memory = TRUE,
  overwrite = TRUE)

save_json(x, path)

load_df(con, name, value, memory = TRUE, repartition = 0,
  overwrite = TRUE)
}
\arguments{
\item{con}{Connection to dplyr source}

\item{name}{Name to reference the data source once it's loaded}

\item{path}{The path to the CSV file. Needs to be accessible from the cluster. Supports: "hdfs://" or "s3n://"}

\item{repartition}{Total of partitions used to distribute table or 0 (default) to avoid partitioning}

\item{memory}{Loads data into memory}

\item{overwrite}{Overwrite the table with the given name when it exists}

\item{x}{A dplyr operation, for instance, `tbls(db, "flights")`}
}
\description{
Loads a CSV file and provides a data source compatible with dplyr

Saves a dplyr operation result as a CSV file

Loads a parquet file and provides a data source compatible with dplyr

Saves dplyr operation result as a parquet file

Loads a JSON file and provides a data source compatible with dplyr

Saves dplyr operation result as a JSON file

Loads a dataframe and provides a data source compatible with dplyr
}

