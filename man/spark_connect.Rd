% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/connection_spark.R
\name{spark_connect}
\alias{spark_connect}
\title{Connects to Spark and establishes the Spark Context}
\usage{
spark_connect(master = "local", app_name = "rspark", version = NULL,
  hadoop_version = NULL, cores = "auto", packages = NULL,
  reconnect = FALSE)
}
\arguments{
\item{master}{Master definition to Spark cluster}

\item{app_name}{Application name to be used while running in the Spark cluster}

\item{version}{Version of the Spark cluster. Use spark_versions() for a list of supported Spark versions.}

\item{hadoop_version}{Version of Hadoop. Use spark_versions_hadoop() for a list of supported Hadoop versions.}

\item{cores}{Cores available for use for Spark. This option is only applicable to local installations. Use NULL
to prevent this package from making use of this parameter and "auto" to default to automatic core detection. Strictly
speaking, this option configures the number of available threads in a local spark instance; however, in practice, the
OS schedules one thread per core.}

\item{packages}{Collection of packages to load into Spark. See also, the rspark.packages.default option.}

\item{reconnect}{Reconnects automatically to Spark on the next attempt to access an Spark resource. This is useful
to support long running services that need to be always connected. This parameter is not supported for local installs.
Reconnect requires the package to not be unloaded, and therefore, is more suitable to improve intermittent connectivity
with Spark.}
}
\description{
Connects to Spark and establishes the Spark Context
}

