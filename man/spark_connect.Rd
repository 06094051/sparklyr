% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/connection_spark.R
\name{spark_connect}
\alias{spark_connect}
\title{Connects to Spark and establishes the Spark Context}
\usage{
spark_connect(master = "local", app_name = "rspark", version = NULL,
  hadoop_version = NULL, config = spark_config())
}
\arguments{
\item{master}{Master definition to Spark cluster}

\item{app_name}{Application name to be used while running in the Spark cluster}

\item{version}{Version of the Spark cluster. Use spark_versions() for a list of supported Spark versions.}

\item{hadoop_version}{Version of Hadoop. Use spark_versions_hadoop() for a list of supported Hadoop versions.}

\item{config}{A list containing configurations settings. This file overrides settings set on config.yml.}
}
\description{
Connects to Spark and establishes the Spark Context
}
\examples{
\dontrun{
 sc <- spark_connect(config = list(
   sql = list(
     spark.sql.shuffle.partitions = 1
   )
 ))
}
}

