% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/api_spark_data.R
\name{spark_read_parquet}
\alias{spark_read_parquet}
\title{Reads a parquet file and provides a data source compatible with dplyr}
\usage{
spark_read_parquet(sc, name, path, repartition = 0, memory = TRUE,
  overwrite = TRUE)
}
\arguments{
\item{sc}{The Spark connection}

\item{name}{Name to reference the data source once it's loaded}

\item{path}{The path to the file. Needs to be accessible from the cluster. Supports: "hdfs://" or "s3n://"}

\item{repartition}{Total of partitions used to distribute table or 0 (default) to avoid partitioning}

\item{memory}{Loads data into memory}

\item{overwrite}{Overwrite the table with the given name when it exists}
}
\description{
Reads a parquet file and provides a data source compatible with dplyr
}

