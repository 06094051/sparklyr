% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ml_feature_transformation.R
\name{sdf_partition}
\alias{sdf_partition}
\title{Partition a Spark Dataframe}
\usage{
sdf_partition(x, ..., weights = NULL, seed = sample(.Machine$integer.max,
  1))
}
\arguments{
\item{x}{An object coercable to a Spark DataFrame.}

\item{...}{Named parameters, mapping table names to weights. The weights
will be normalized such that they sum to 1.}

\item{weights}{An alternate mechanism for supplying weights -- when
specified, this takes precedence over the \code{...} arguments.}

\item{seed}{Random seed to use for randomly partitioning the dataset. Set
this if you want your partitioning to be reproducible on repeated runs.}
}
\value{
An \R \code{list} of \code{tbl_spark}s.
}
\description{
Partition a Spark DataFrame into multiple groups. This routine is useful
for splitting a DataFrame into, for example, training and test datasets.
}
\details{
The sampling weights define the probability that a particular observation
will be assigned to a particular partition, not the resulting size of the
partition. This implies that partitioning a DataFrame with, for example,

\code{sdf_partition(x, training = 0.5, test = 0.5)}

is not guaranteed to produce \code{training} and \code{test} partitions
of equal size.
}
\section{Transforming Spark DataFrames}{


The family of functions prefixed with \code{sdf_} generally
access the Scala Spark DataFrame API directly, and return a
lower-level Spark DataFrame object (represented in \R as a
\code{sparkapi_jobj}). To bring these back to the \code{dplyr}
world (in order to use, for example, \code{mutate}), you need
to invoke \code{\link{sdf_register}()}. This gives the Spark
DataFrame a table name in the associated Spark SQL context, and
then returns a \code{tbl_spark} which you can then use with
the \code{dplyr} interface.
}
\examples{
\dontrun{
# randomly partition data into a 'training' and 'test'
# dataset, with 60\% of the observations assigned to the
# 'training' dataset, and 40\% assigned to the 'test' dataset
data(diamonds, package = "ggplot2")
diamonds_tbl <- copy_to(sc, diamonds, "diamonds")
partitions <- diamonds_tbl \%>\%
  sdf_partition(training = 0.6, test = 0.4)
print(partitions)

# alternate way of specifying weights
weights <- c(training = 0.6, test = 0.4)
diamonds_tbl \%>\% sdf_partition(weights = weights)
}
}
\seealso{
Other Spark DataFrame functions: \code{\link{sdf_sample}},
  \code{\link{sdf_sort}}
}

