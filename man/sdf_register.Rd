% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sdf_interface.R
\name{sdf_register}
\alias{sdf_register}
\title{Register a Spark DataFrame}
\usage{
sdf_register(x, name)
}
\arguments{
\item{x}{A Spark DataFrame.}

\item{name}{A name to assign this table.}
}
\description{
Registers a Spark DataFrame (giving it a table name for the
Spark SQL context), and returns a \code{tbl_spark}.
}
\section{Transforming Spark DataFrames}{


The family of functions prefixed with \code{sdf_} generally
access the Scala Spark DataFrame API directly, and return a
lower-level Spark DataFrame object (represented in \R as a
\code{sparkapi_jobj}). To bring these back to the \code{dplyr}
world (in order to use, for example, \code{mutate}), you need
to invoke \code{\link{sdf_register}()}. This gives the Spark
DataFrame a table name in the associated Spark SQL context, and
then returns a \code{tbl_spark} which you can then use with
the \code{dplyr} interface.
}
\seealso{
Other Spark data frames: \code{\link{sdf_collect}},
  \code{\link{sdf_copy_to}}, \code{\link{sdf_mutate}},
  \code{\link{sdf_partition}}, \code{\link{sdf_predict}},
  \code{\link{sdf_sample}}, \code{\link{sdf_sort}}
}

