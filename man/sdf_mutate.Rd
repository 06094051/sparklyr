% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sdf_interface.R
\name{sdf_mutate}
\alias{sdf_mutate}
\alias{sdf_mutate_}
\title{Mutate a Spark DataFrame}
\usage{
sdf_mutate(.data, ...)

sdf_mutate_(.data, ..., .dots)
}
\arguments{
\item{.data}{A \code{spark_tbl}.}

\item{...}{Named arguments, mapping new column names to the transformation to
be applied.}

\item{.dots}{A named list, mapping output names to transformations.}
}
\description{
Use Spark's \href{http://spark.apache.org/docs/latest/ml-features.html}{feature transformers}
to mutate a Spark DataFrame.
}
\details{
\code{sdf_mutate()} differs from \code{mutate} in a number of important ways:

\itemize{

\item \code{mutate} returns a \code{tbl_spark}, while \code{sdf_mutate} returns
  a Spark DataFrame (represented by a \code{jobj}),
  
\item \code{mutate} works 'lazily' (the generated SQL is not evaluated until \code{collect}
  is called), while \code{sdf_mutate} works 'eagerly' (the feature transformer, as well as
  and pending SQL from a previous pipeline, is applied),
  
\item To transform the Spark DataFrame back to a \code{tbl_spark}, you should
  use \code{\link{sdf_register}}.
 
}

Overall, this implies that if you wish to mix a \code{dplyr} pipeline with \code{sdf_mutate},
you should generally apply your \code{dplyr} pipeline first, then finalize your output with
\code{sdf_mutate}. See \strong{Examples} for an example of how this might be done.
}
\section{Transforming Spark DataFrames}{


The family of functions prefixed with \code{sdf_} generally
access the Scala Spark DataFrame API directly, and return a
lower-level Spark DataFrame object (represented in \R as a
\code{sparkapi_jobj}). To bring these back to the \code{dplyr}
world (in order to use, for example, \code{mutate}), you need
to invoke \code{\link{sdf_register}()}. This gives the Spark
DataFrame a table name in the associated Spark SQL context, and
then returns a \code{tbl_spark} which you can then use with
the \code{dplyr} interface.
}
\examples{
\dontrun{
# using the 'beaver1' dataset, binarize the 'temp' column
data(beavers, package = "datasets")
beaver_tbl <- copy_to(sc, beaver1, "beaver")
beaver_tbl \%>\%
  mutate(squared = temp ^ 2) \%>\%
  sdf_mutate(warm = ft_binarizer(squared, 1000)) \%>\%
  sdf_register("mutated")

# view our newly constructed tbl
head(beaver_tbl)

# note that we have two separate tbls registered
dplyr::src_tbls(sc)
}
}
\seealso{
Other Spark data frames: \code{\link{sdf_collect}},
  \code{\link{sdf_copy_to}}, \code{\link{sdf_partition}},
  \code{\link{sdf_predict}}, \code{\link{sdf_register}},
  \code{\link{sdf_sample}}, \code{\link{sdf_sort}}

Other feature transformation routines: \code{\link{ft_binarizer}},
  \code{\link{ft_bucketizer}},
  \code{\link{ft_discrete_cosine_transform}},
  \code{\link{ft_elementwise_product}},
  \code{\link{ft_index_to_string}},
  \code{\link{ft_quantile_discretizer}},
  \code{\link{ft_sql_transformer}},
  \code{\link{ft_string_indexer}},
  \code{\link{ft_vector_assembler}}
}

