#' Feature Transformation -- VectorAssembler
#'
#' Combine multiple vectors into a single row-vector; that is,
#' where each row element of the newly generated column is a
#' vector formed by concatenating each row element from the
#' specified input columns.
#'
#' @template ml-transformation
#'
#' @export
ml_apply_vector_assembler <- function(x, input_col, output_col)
{
  df <- as_spark_dataframe(x)
  scon <- spark_scon(df)

  assembler <- spark_invoke_static_ctor(
    scon,
    "org.apache.spark.ml.feature.VectorAssembler"
  )

  assembler %>%
    spark_invoke("setInputCols", as.list(input_col)) %>%
    spark_invoke("setOutputCol", output_col) %>%
    spark_invoke("transform", df)
}

#' Feature Transformation -- StringIndexer
#'
#' Encode a column of labels into a column of label indices.
#' The indices are in [0, numLabels), ordered by label frequencies, with
#' the most frequent label assigned index 0. The transformation
#' can be reversed with \code{\link{ml_apply_index_to_string}}.
#'
#' @template ml-transformation
#'
#' @param params An (optional) \R environment -- when available,
#'   the index <-> label mapping generated by the string indexer
#'   will be injected into this environment under the \code{labels}
#'   key.
#'
#' @export
ml_apply_string_indexer <- function(x, input_col, output_col,
                                    params = NULL)
{
  df <- as_spark_dataframe(x)
  scon <- spark_scon(df)

  indexer <- spark_invoke_static_ctor(
    scon,
    "org.apache.spark.ml.feature.StringIndexer"
  )

  sim <- indexer %>%
    spark_invoke("setInputCol", input_col) %>%
    spark_invoke("setOutputCol", output_col) %>%
    spark_invoke("fit", df)

  # Report labels to caller if requested -- these map
  # the discovered labels in the data set to an associated
  # index.
  if (is.environment(params))
    params$labels <- as.character(spark_invoke(sim, "labels"))

  spark_invoke(sim, "transform", df)
}

#' Feature Transformation -- Binarizer
#'
#' Apply thresholding to a column, such that values less than or equal to the
#' \code{threshold} are assigned the value 0.0, and values greater than the
#' threshold are assigned the value 1.0.
#'
#' @template ml-transformation
#'
#' @param threshold The numeric threshold.
#'
#' @export
ml_apply_binarizer <- function(x, input_col, output_col,
                               threshold = 0.5)
{
  df <- as_spark_dataframe(x)
  scon <- spark_scon(df)

  binarizer <- spark_invoke_static_ctor(
    scon,
    "org.apache.spark.ml.feature.Binarizer"
  )

  binarizer %>%
    spark_invoke("setInputCol", input_col) %>%
    spark_invoke("setOutputCol", output_col) %>%
    spark_invoke("setThreshold", as.double(threshold)) %>%
    spark_invoke("transform", df)
}

#' Feature Transformation -- Discrete Cosine Transform (DCT)
#'
#' Transform a column in the time domain into another column in the frequency
#' domain.
#'
#' @template ml-transformation
#'
#' @param inverse Perform inverse DCT?
#'
#' @export
ml_apply_discrete_cosine_transform <- function(x, input_col, output_col,
                                               inverse = FALSE)
{
  df <- as_spark_dataframe(x)
  scon <- spark_scon(df)

  dct <- spark_invoke_static_ctor(
    scon,
    "org.apache.spark.ml.feature.DCT"
  )

  dct %>%
    spark_invoke("setInputCol", input_col) %>%
    spark_invoke("setOutputCol", output_col) %>%
    spark_invoke("setInverse", as.logical(inverse)) %>%
    spark_invoke("transform", df)
}

#' Feature Transformation -- IndexToString
#'
#' Symmetrically to \code{\link{ml_apply_string_indexer}},
#' \code{ml_apply_index_to_string} maps a column of label indices back to a
#' column containing the original labels as strings.
#'
#' @template ml-transformation
#'
#' @export
ml_apply_index_to_string <- function(x, input_col, output_col)
{
  df <- as_spark_dataframe(x)
  scon <- spark_scon(df)

  converter <- spark_invoke_static_ctor(
    scon,
    "org.apache.spark.ml.feature.IndexToString"
  )

  converter %>%
    spark_invoke("setInputCol", input_col) %>%
    spark_invoke("setOutputCol", output_col) %>%
    spark_invoke("transform", df)
}

## TODO: These routines with so-called 'row vector' features by
## default, but it would be much nicer to implement routines to
## scale whole columns instead.
# ml_apply_standard_scaler <- function(df, input_col, output_col,
#                                      with.mean, with.std)
# {
#   scon <- spark_scon(df)
#
#   scaler <- spark_invoke_static_ctor(
#     scon,
#     "org.apache.spark.ml.feature.StandardScaler"
#   )
#
#   scaler %>%
#     spark_invoke("setInputCol", input_col) %>%
#     spark_invoke("setOutputCol", output_col) %>%
#     spark_invoke("setWithMean", as.logical(with.mean)) %>%
#     spark_invoke("setWithStd", as.logical(with.std)) %>%
#     spark_invoke("transform", df)
# }
#
# ml_apply_min_max_scaler <- function(df, input_col, output_col,
#                                     min = 0, max = 1)
# {
#   scon <- spark_scon(df)
#
#   scaler <- spark_invoke_static_ctor(
#     scon,
#     "org.apache.spark.ml.feature.MinMaxScaler"
#   )
#
#   scaler %>%
#     spark_invoke("setInputCol", input_col) %>%
#     spark_invoke("setOutputCol", output_col) %>%
#     spark_invoke("setMin", as.numeric(min)) %>%
#     spark_invoke("setMax", as.numeric(max)) %>%
#     spark_invoke("transform", df)
# }

#' Feature Transformation -- Bucketizer
#'
#' Similar to \R's \code{\link{cut}} function, this transforms a numeric column
#' into a discretized column, with breaks specified through the \code{splits}
#' parameter.
#'
#' @template ml-transformation
#'
#' @param splits A numeric vector of cutpoints, indicating the bucket
#'   boundaries.
#'
#' @export
ml_apply_bucketizer <- function(x, input_col, output_col,
                                splits)
{
  df <- as_spark_dataframe(x)
  scon <- spark_scon(df)

  bucketizer <- spark_invoke_static_ctor(
    scon,
    "org.apache.spark.ml.feature.Bucketizer"
  )

  bucketizer %>%
    spark_invoke("setInputCol", input_col) %>%
    spark_invoke("setOutputCol", output_col) %>%
    spark_invoke("setSplits", as.list(splits)) %>%
    spark_invoke("transform", df)
}

#' Feature Transformation -- ElementwiseProduct
#'
#' Computes the element-wise product between two columns. Generally, this is
#' intended as a scaling transformation, where an input vector is scaled by
#' another vector, but this should apply for all element-wise product
#' transformations.
#'
#' @template ml-transformation
#'
#' @param scaling_col The column used to scale \code{input_col}.
#'
#' @export
ml_apply_elementwise_product <- function(x, input_col, output_col, scaling_col)
{
  df <- as_spark_dataframe(x)
  scon <- spark_scon(df)

  transformer <- spark_invoke_static_ctor(
    scon,
    "org.apache.spark.ml.feature.ElementwiseProduct"
  )

  transformer %>%
    spark_invoke("setInputCol", input_col) %>%
    spark_invoke("setOutputCol", output_col) %>%
    spark_invoke("setScalingVec", scaling_col) %>%
    spark_invoke("transform", df)
}

#' Feature Transformation -- SQLTransformer
#'
#' Transform a data set using SQL. Use the \code{__THIS__}
#' placeholder as a proxy for the active table.
#'
#' @template ml-transformation
#'
#' @param sql A SQL statement.
#'
#' @export
ml_apply_sql_transformer <- function(x, input_col, output_col, sql)
{
  df <- as_spark_dataframe(x)
  scon <- spark_scon(df)

  transformer <- spark_invoke_static_ctor(
    scon,
    "org.apache.spark.ml.feature.SQLTransformer"
  )

  transformer %>%
    spark_invoke("setStatement", paste(sql, collapse = "\n")) %>%
    spark_invoke("transform", df)
}

#' Feature Transformation -- QuantileDiscretizer
#'
#' Takes a column with continuous features and outputs a column with binned
#' categorical features. The bin ranges are chosen by taking a sample of the
#' data and dividing it into roughly equal parts. The lower and upper bin bounds
#' will be -Infinity and +Infinity, covering all real values. This attempts to
#' find numBuckets partitions based on a sample of the given input data, but it
#' may find fewer depending on the data sample values.
#'
#' Note that the result may be different every time you run it, since the sample
#' strategy behind it is non-deterministic.
#'
#' @template ml-transformation
#'
#' @param n_buckets The number of buckets to use.
#'
#' @export
ml_apply_quantile_discretizer <- function(x, input_col, output_col,
                                          n_buckets = 5)
{
  df <- as_spark_dataframe(x)
  scon <- spark_scon(df)

  discretizer <- spark_invoke_static_ctor(
    scon,
    "org.apache.spark.ml.feature.QuantileDiscretizer"
  )

  discretizer %>%
    spark_invoke("setInputCol", input_col) %>%
    spark_invoke("setOutputCol", output_col) %>%
    spark_invoke("setNumBuckets", as.numeric(n_buckets)) %>%
    spark_invoke("fit", df) %>%
    spark_invoke("transform", df)
}
